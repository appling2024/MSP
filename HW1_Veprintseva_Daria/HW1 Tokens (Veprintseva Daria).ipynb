{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa7ccd-5446-4951-aef5-5d192a6bf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/veprincevadasa/article_godunov.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0388d-92f9-449c-9a94-633a460de741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация с NLTK\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(text, language='russian')\n",
    "\n",
    "with open('/Users/veprincevadasa/tokens_nltk.txt', 'w', encoding='utf-8') as output_file:\n",
    "   for token in nltk_tokens:\n",
    "       output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c09d18-cbc0-4535-b53e-059158693c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Токенизация с Stanza\n",
    "\n",
    "import stanza\n",
    "stanza.download('ru')\n",
    "\n",
    "stanza_nlp = stanza.Pipeline('ru')\n",
    "stanza_doc = nlp(text)\n",
    "stanza_tokens = [word.text for sentence in stanza_doc.sentences for word in sentence.words]\n",
    "\n",
    "with open('/Users/veprincevadasa/tokens_stanza.txt', 'w', encoding='utf-8') as output_file:\n",
    "   for token in stanza_tokens:\n",
    "       output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51734775-1688-4b61-9b04-8dfa8fa1f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from razdel import tokenize\n",
    "razdel_tokens = list(tokenize(text))\n",
    "razdel_doc = [token.text for token in razdel_tokens]\n",
    "with open('/Users/veprincevadasa/tokens_razdel.txt', 'w', encoding='utf-8') as output_file:\n",
    "   for token in razdel_doc:\n",
    "       output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ae037-420b-46fc-acf1-8562778767e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация с segtok НЕ РАБОТАЕТ\n",
    "\n",
    "# !pip install segtok\n",
    "\n",
    "# from segtok import tokenizer\n",
    "\n",
    "# with open('/Users/veprincevadasa/article_godunov.txt', 'r', encoding='utf-8') as file:\n",
    "#     text = file.read()\n",
    "    \n",
    "# segtok_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# with open('/Users/veprincevadasa/tokens_segtok.txt', 'w', encoding='utf-8') as output_file:\n",
    "#    for token in segtok_tokens:\n",
    "#        output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce1a06-d85f-4082-8379-377b03b770f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация с spacy НЕ РАБОТАЕТ\n",
    "# import spacy\n",
    "# spacy_nlp = spacy.load('/Users/veprincevadasa/myenv/lib/python3.12/site-packages/ru_core_news_sm')\n",
    "\n",
    "# with open('/Users/veprincevadasa/article_godunov.txt', 'r', encoding='utf-8') as file:\n",
    "#     text = file.read()\n",
    "    \n",
    "# spacy_doc = spacy_nlp(text)\n",
    "# spacy_tokens = [token.text for token in spacy_doc]\n",
    "# print(\"SpaCy:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb98f2f-4fd4-4df8-9314-a6c816c71748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses\n",
    "from sacremoses import MosesTokenizer\n",
    "tokenizer = MosesTokenizer(lang='ru')\n",
    "tokens_moses = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokens (Moses):\", tokens_moses)\n",
    "with open('/Users/veprincevadasa/tokens_moses.txt', 'w', encoding='utf-8') as output_file:\n",
    "   for token in tokens_moses:\n",
    "       output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd6c5e-2c8b-4280-bb29-c64ee72f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymorphy3\n",
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "tokens_pymorphy = [morph.parse(word)[0].normal_form for word in text.split()]\n",
    "with open('/Users/veprincevadasa/tokens_pymorphy.txt', 'w', encoding='utf-8') as output_file:\n",
    "   for token in tokens_pymorphy:\n",
    "       output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "006a008a-abc8-455d-b74b-bc021dc21a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.udpipe in /opt/anaconda3/lib/python3.12/site-packages (1.3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Токенизация с udpipe\n",
    "!pip install ufal.udpipe\n",
    "import ufal.udpipe\n",
    "model_path = '/Users/veprincevadasa/Downloads/russian-syntagrus-ud-2.5-191206.udpipe'\n",
    "model = ufal.udpipe.Model.load(model_path)\n",
    "\n",
    "pipeline = ufal.udpipe.Pipeline(model, 'tokenize', 'true', 'false', 'conllu')\n",
    "udpipe_output = pipeline.process(text)\n",
    "\n",
    "tokens_udpipe = []\n",
    "for line in udpipe_output.split('\\n'):\n",
    "    if line and not line.startswith('#'):  # Игнорируем пустые строки и комментарии\n",
    "        parts = line.split('\\t')\n",
    "        tokens_udpipe.append(parts[1])  # Добавляем только токен (второй элемент)\n",
    "\n",
    "\n",
    "with open('/Users/veprincevadasa/tokens_udpipe.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for token in tokens_udpipe:\n",
    "        output_file.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06666bbd-aec7-4e87-bc03-8cc33a3e424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokens(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read().strip().splitlines()\n",
    "\n",
    "file_paths = [\n",
    "    '/Users/veprincevadasa/tokenization/tokens_nltk.txt',\n",
    "    '/Users/veprincevadasa/tokenization/tokens_stanza.txt',\n",
    "    '/Users/veprincevadasa/tokenization/tokens_razdel.txt',\n",
    "    '/Users/veprincevadasa/tokenization/tokens_moses.txt',\n",
    "    '/Users/veprincevadasa/tokenization/tokens_udpipe.txt',\n",
    "    '/Users/veprincevadasa/tokenization/tokens_pymorphy.txt'\n",
    "]\n",
    "\n",
    "\n",
    "def compare_multiple_files(file_paths):\n",
    "    all_tokens = {}\n",
    "    unique_tokens = {file_path: [] for file_path in file_paths}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        all_tokens[file_path] = read_tokens(file_path)\n",
    "\n",
    "    for file_path, tokens in all_tokens.items():\n",
    "        unique = [token for token in tokens if not any(token in other_tokens for other_tokens in all_tokens.values() if other_tokens != tokens)]\n",
    "        unique_tokens[file_path] = unique\n",
    "\n",
    "    return unique_tokens\n",
    "\n",
    "unique_tokens = compare_multiple_files(file_paths)\n",
    "\n",
    "for file_path, tokens in unique_tokens.items():\n",
    "    print(f\"Токены только в {file_path}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc178f-e858-4d9b-9df5-1ea641109974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводы\n",
    "\n",
    "# nltk: простой и удобный способ; ошибок нет;\n",
    "# tokens_moses: делит некоторые слова (напр., привле/кательность);\n",
    "# udpipe: не выделяет знак '«' как отдельный токен, привязвает его к слову (напр., «Петербургского/листка»;\n",
    "# stanza: ошибок нет;\n",
    "# pymorphy: не нашла код для проведения только токенизации; вместе с токенизацией провел лемматизацию, но не для всех слов (почему-то…); не выделяет знак « как отдельный токен; самый неудачный результат токенизации;\n",
    "# razdel: простой и удобный способ; ошибок нет."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
